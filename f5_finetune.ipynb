{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\n",
    "# from datasets import Dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import json\n",
    "import json_lines\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultipleChoiceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_riddlesense_prompt(question, options):\n",
    "    prompt = \\\n",
    "\"\"\"\n",
    "Question: {}\n",
    "\n",
    "What is the correct answer to the question from the following choices?\n",
    "Options: \n",
    "(A): {}\n",
    "(B): {}\n",
    "(C): {}\n",
    "(D): {}\n",
    "(E): {}\"\"\".format(question, options[0], options[1], options[2], options[3], options[4])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'rb') as f: \n",
    "        for item in json_lines.reader(f):\n",
    "            data.append(item)\n",
    "\n",
    "    processed_data = []\n",
    "    for item in data:\n",
    "        question = item['question']['stem']\n",
    "        options = [_['text'] for _ in item['question']['choices']]\n",
    "        answer = item['answerKey']\n",
    "        text = get_riddlesense_prompt(question, options)\n",
    "\n",
    "        model_inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "        )\n",
    "\n",
    "        # labels = tokenizer(answer, max_length=2, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        labels = tokenizer(answer, max_length=2, padding=\"max_length\", truncation=True)\n",
    "        labels = labels[\"input_ids\"]\n",
    "        labels = [l if l != tokenizer.pad_token_id else -100 for l in labels]\n",
    "        model_inputs[\"labels\"] = labels\n",
    "        processed_data.append(model_inputs)\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'google/flan-t5-small'\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_data(\"data/rs_train.jsonl\")\n",
    "valid_data = load_data(\"data/rs_dev.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MultipleChoiceDataset(train_data)\n",
    "valid_dataset = MultipleChoiceDataset(valid_data)\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "train_loader = DataLoader(train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_dataset, shuffle=False, collate_fn=default_data_collator, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "lr = 1e-3\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_loader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.56it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 58.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(2.2439, device='cuda:0') train_epoch_loss=tensor(0.8082, device='cuda:0') eval_ppl=tensor(2.2364, device='cuda:0') eval_epoch_loss=tensor(0.8049, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.37it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 59.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(2.2439, device='cuda:0') train_epoch_loss=tensor(0.8082, device='cuda:0') eval_ppl=tensor(2.2391, device='cuda:0') eval_epoch_loss=tensor(0.8061, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.48it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 59.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(2.2441, device='cuda:0') train_epoch_loss=tensor(0.8083, device='cuda:0') eval_ppl=tensor(2.2369, device='cuda:0') eval_epoch_loss=tensor(0.8051, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.53it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 59.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(2.2419, device='cuda:0') train_epoch_loss=tensor(0.8073, device='cuda:0') eval_ppl=tensor(2.2392, device='cuda:0') eval_epoch_loss=tensor(0.8061, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.53it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 59.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(2.2463, device='cuda:0') train_epoch_loss=tensor(0.8093, device='cuda:0') eval_ppl=tensor(2.2404, device='cuda:0') eval_epoch_loss=tensor(0.8067, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.50it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 60.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl=tensor(2.2386, device='cuda:0') train_epoch_loss=tensor(0.8059, device='cuda:0') eval_ppl=tensor(2.2424, device='cuda:0') eval_epoch_loss=tensor(0.8076, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.49it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 59.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl=tensor(2.2403, device='cuda:0') train_epoch_loss=tensor(0.8066, device='cuda:0') eval_ppl=tensor(2.2370, device='cuda:0') eval_epoch_loss=tensor(0.8051, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.32it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 60.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: train_ppl=tensor(2.2388, device='cuda:0') train_epoch_loss=tensor(0.8060, device='cuda:0') eval_ppl=tensor(2.2372, device='cuda:0') eval_epoch_loss=tensor(0.8052, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.54it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 60.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: train_ppl=tensor(2.2391, device='cuda:0') train_epoch_loss=tensor(0.8061, device='cuda:0') eval_ppl=tensor(2.2363, device='cuda:0') eval_epoch_loss=tensor(0.8048, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 878/878 [00:53<00:00, 16.55it/s]\n",
      "100%|██████████| 256/256 [00:04<00:00, 60.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: train_ppl=tensor(2.2378, device='cuda:0') train_epoch_loss=tensor(0.8055, device='cuda:0') eval_ppl=tensor(2.2369, device='cuda:0') eval_epoch_loss=tensor(0.8051, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_loader)):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    true_preds = []\n",
    "    for step, batch in enumerate(tqdm(valid_loader)):\n",
    "        inputs = {key: val.to(device) for key, val in batch.items() if key != \"labels\"}\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "        true_preds.extend(\n",
    "            tokenizer.batch_decode(batch[\"labels\"].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(valid_loader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_loader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")\n",
    "\n",
    "# model.save_pretrained(f'{model_name[7:]}_finetuned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy=20.372184133202744 % on the evaluation dataset\n",
      "eval_preds[:10]=['D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D', 'D']\n",
      "true_preds[:10]=['B', 'E', 'A', 'D', 'B', 'D', 'E', 'A', 'A', 'C']\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for pred, true in zip(eval_preds, true_preds):\n",
    "    if pred.strip() == true.strip():\n",
    "        correct += 1\n",
    "    total += 1\n",
    "accuracy = correct / total * 100\n",
    "print(f\"{accuracy=} % on the evaluation dataset\")\n",
    "print(f\"{eval_preds[:10]=}\")\n",
    "print(f\"{true_preds[:10]=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n",
    "# model.push_to_hub(\"devanshrj/t5-large_PREFIX_TUNING_SEQ2SEQ\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brainteaser_prompt(question, options):\n",
    "    prompt = \\\n",
    "\"\"\"\n",
    "Question: {}\n",
    "\n",
    "What is the correct answer to the question from the following choices?\n",
    "Options: \n",
    "(A): {}\n",
    "(B): {}\n",
    "(C): {}\n",
    "(D): {}\"\"\".format(question, options[0], options[1], options[2], options[3])\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Mr. and Mrs. Mustard have six daughters and each daughter has one brother. But there are only 9 people in the family, how is that possible?\"\n",
    "options = [\"Some daughters get married and have their own family.\", \"Each daughter shares the same brother.\", \"Some brothers were not loved by family and moved away.\", \"None of above.\"]\n",
    "bt_prompt = get_brainteaser_prompt(question, options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(bt_prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['(C)']\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"], max_new_tokens=10)\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
